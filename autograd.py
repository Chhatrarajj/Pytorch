# -*- coding: utf-8 -*-
"""Autograd.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kPr30ZqdR8Yp5aKi62Vy-cdebJHrSsXo
"""

def dy_dx(x):
  return 2*x

dy_dx(2)

import math as m
def dz_dx(x):
  return 2*x*m.cos(x**2)

dz_dx(3)

import torch

x = torch.tensor(3.0,requires_grad=True)

y=x**2

z= torch.sin(y)

x

y

z

# imp
z.backward()

# imp
print(x.grad)

x= torch.tensor(6.7)
y=torch.tensor(0.0)

w=torch.tensor(1.0)
b=torch.tensor(0.0)

def binary_cross_entropy_loss(prediction,target):
  epsilon=1e-8
  prediction = torch.clamp(prediction,epsilon,1-epsilon)
  return -(target*torch.log(prediction)+(1 - target)*torch.log(1- prediction))

#forward pass
z= w * x + b
y_pred = torch.sigmoid(z)

loss = binary_cross_entropy_loss(y_pred,y)

loss

dloss_dy_pred = (y_pred - y)/(y_pred * (1 - y_pred))

dy_pred_dz = y_pred * (1 - y_pred)

dz_dw=x
dz_db=1

dl_dw = dloss_dy_pred * dy_pred_dz * dz_dw
dl_db = dloss_dy_pred * dy_pred_dz * dz_db

print(dl_dw)
print(dl_db)

x = torch.tensor(6.7)
y  = torch.tensor(0.0)
w = torch.tensor(1.0,requires_grad=True)
b = torch.tensor(0.0,requires_grad=True)

x

y

w

b

z = w *x + b

y_pred= torch.sigmoid(z)

y_pred

loss = binary_cross_entropy_loss(y_pred,y)

loss

loss.backward()

print(w.grad)
print(b.grad)

x = torch.tensor([1.0,2.0,3.0],requires_grad=True)
x

y = (x**2).mean()
y

y.backward()
print(x.grad)

#clearing gradients
x.grad.zero_()

x.requires_grad(False)

x.detach()

with torch.no_grad():
  y = x**2
  print(y)

